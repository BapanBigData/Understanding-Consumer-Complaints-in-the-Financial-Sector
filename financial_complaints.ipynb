{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Estimator\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, abs, desc, rand\n",
    "from pyspark.sql.types import TimestampType, LongType\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import Param, Params, TypeConverters, HasOutputCols, HasInputCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyspark Version: 3.5.2\n",
      "Spark Session created successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pyspark Version: {pyspark.__version__}\")\n",
    "\n",
    "# create a spark session\n",
    "spark_session = SparkSession.builder \\\n",
    "        .appName(\"financial_complaints_app\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.master\", \"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# check if the spark session created successfully\n",
    "print(\"Spark Session created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-1G0D7HE:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>financial_complaints_app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1dfdbe85e50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='undefined', name='getfrequencyInfo', doc='getfrequencyInfo')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencyInfo = Param(\n",
    "    Params._dummy(),\n",
    "    \"getfrequencyInfo\",\n",
    "    \"getfrequencyInfo\",\n",
    "    typeConverter=TypeConverters.toList\n",
    ")\n",
    "\n",
    "frequencyInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FrequencyEncoder - An Estimator that calculates the frequency of each unique category in the specified columns\n",
    "\n",
    "class FrequencyEncoder(Estimator, HasInputCols, HasOutputCols,\n",
    "                        DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    # Defines a parameter to store frequency information (list of category frequencies)\n",
    "    frequencyInfo = Param(\n",
    "        Params._dummy(),\n",
    "        \"getfrequencyInfo\",\n",
    "        \"getfrequencyInfo\",\n",
    "        typeConverter=TypeConverters.toList\n",
    "    )\n",
    "    \n",
    "    # Initialization\n",
    "    def __init__(self, inputCols: list[str], outputCols: list[str]):\n",
    "        super().__init__()\n",
    "        self._setDefault(frequencyInfo=[])\n",
    "        self.setParams(inputCols=inputCols, outputCols=outputCols)\n",
    "    \n",
    "    # Sets the frequency info after itâ€™s calculated in the _fit method\n",
    "    def setfrequencyInfo(self, frequencyInfo: list):\n",
    "        return self._set(frequencyInfo=frequencyInfo)\n",
    "    \n",
    "    \n",
    "    def getfrequencyInfo(self):\n",
    "        return self.getOrDefault(self.frequencyInfo)\n",
    "    \n",
    "    \n",
    "    def setParams(self, inputCols: list[str], outputCols: list[str]):\n",
    "        return self._set(inputCols=inputCols, outputCols=outputCols)\n",
    "    \n",
    "    \n",
    "    def _fit(self, dataframe: DataFrame):\n",
    "        input_columns = self.getInputCols()\n",
    "        output_columns = self.getOutputCols()\n",
    "        replace_info = []  # list to hold frequency information\n",
    "        \n",
    "        # for each column calculate frequency and collect the data\n",
    "        for column, new_column in zip(input_columns, output_columns):\n",
    "            freq = dataframe.select(col(column).alias(f\"g_{column}\")) \\\n",
    "                            .groupBy(col(f\"g_{column}\")).count() \\\n",
    "                            .withColumn(new_column, col(\"count\"))\n",
    "            \n",
    "            freq = freq.drop(\"count\") # Drop unnecessary count column\n",
    "            replace_info.append(freq.collect())\n",
    "            \n",
    "        # Set frequency info in the Estimator\n",
    "        self.setfrequencyInfo(replace_info)\n",
    "        \n",
    "        return FrequencyEncoderModel(inputCols=input_columns, outputCols=output_columns).setfrequencyInfo(replace_info)\n",
    "    \n",
    "    \n",
    "\n",
    "# FrequencyEncoderModel - A Transformer that uses the frequency info to encode categorical values\n",
    "class FrequencyEncoderModel(FrequencyEncoder, Transformer):\n",
    "    \n",
    "    def __init__(self, inputCols: list[str], outputCols: list[str]):\n",
    "        super().__init__(inputCols, outputCols)\n",
    "    \n",
    "    # Transform method to encode columns with their calculated frequencies    \n",
    "    def _transform(self, dataframe: DataFrame):\n",
    "        input_columns = self.getInputCols()\n",
    "        output_columns = self.getOutputCols()\n",
    "        freqInfo = self.getfrequencyInfo()  # retrive the frequency info\n",
    "        \n",
    "        for in_col, out_col, freq_info in zip(input_columns, output_columns, freqInfo):\n",
    "            frequency_dataframe = spark_session.createDataFrame(freq_info) # convert list to dataframe\n",
    "            columns = frequency_dataframe.columns\n",
    "            dataframe = dataframe.join(frequency_dataframe, dataframe[in_col] == frequency_dataframe[columns[0]])\n",
    "            dataframe = dataframe.drop(columns[0]).withColumn(out_col, col(columns[1]))\n",
    "        \n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DerivedFeatureGenerator - A Transformer that generates new features based on time differences\n",
    "\n",
    "class DerivedFeatureGenerator(Transformer, HasInputCols, HasOutputCols,\n",
    "                                DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    def __init__(self, inputCols: list[str] = None, outputCols: list[str] = None):\n",
    "        super().__init__()\n",
    "        self.second_within_day = 24 * 60 * 60 # seconds in a day\n",
    "        self.setParams(inputCols=inputCols, outputCols=outputCols)\n",
    "        \n",
    "    def setParams(self, inputCols: list[str], outputCols: list[str]):\n",
    "        return self._set(inputCols=inputCols, outputCols=outputCols)\n",
    "    \n",
    "    def _transform(self, dataframe: DataFrame):\n",
    "        inputCols = self.getInputCols()\n",
    "        \n",
    "        # Convert columns to timestamp, then calculate difference in days\n",
    "        dataframe = dataframe.withColumn(inputCols[0], col(inputCols[0]).cast(TimestampType()))\n",
    "        dataframe = dataframe.withColumn(inputCols[1], col(inputCols[1]).cast(TimestampType()))\n",
    "        \n",
    "        dataframe = dataframe.withColumn(self.getOutputCols()[0], \n",
    "                                        abs(col(inputCols[1]).cast(LongType()) - col(inputCols[0]).cast(LongType())) / self.second_within_day)\n",
    "        \n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FrequencyImputer - An Estimator that imputes missing values using the most frequent category in each column\n",
    "\n",
    "class FrequencyImputer(Estimator, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    \n",
    "    topCategories = Param(\n",
    "        Params._dummy(),\n",
    "        \"getTopCategories\",\n",
    "        \"Stores most frequent categories\",\n",
    "        typeConverter=TypeConverters.toList\n",
    "    )\n",
    "    \n",
    "    def __init__(self, inputCols: list[str], outputCols: list[str]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._setDefault(topCategories=[])\n",
    "        self.setParams(inputCols=inputCols, outputCols=outputCols)\n",
    "        \n",
    "    def setTopCategories(self, values: list[str]):\n",
    "        return self._set(topCategories=values)\n",
    "    \n",
    "    def getTopCategories(self):\n",
    "        return self.getOrDefault(self.topCategories)\n",
    "    \n",
    "    def setParams(self, inputCols: list[str], outputCols: list[str]):\n",
    "        return self._set(inputCols=inputCols, outputCols=outputCols)\n",
    "    \n",
    "    def _fit(self, dataframe: DataFrame):\n",
    "        inputCols = self.inputCols()\n",
    "        topCategories = []\n",
    "        \n",
    "        for column in inputCols:\n",
    "            most_common = dataframe.groupBy(column) \\\n",
    "                                    .count().orderBy(desc('count')).first()[column]\n",
    "            \n",
    "            topCategories.append(most_common)\n",
    "        \n",
    "        self.setTopCategories(topCategories)\n",
    "        \n",
    "        # Return a Transformer (FrequencyImputerModel) with top categories set\n",
    "        \n",
    "        return FrequencyImputerModel(inputCols=self.getInputCols(), outputCols=self.getOutputCols()).setTopCategories(topCategories)\n",
    "\n",
    "\n",
    "# FrequencyImputerModel - A Transformer that fills missing values with the most frequent categories\n",
    "class FrequencyImputerModel(FrequencyImputer, Transformer):\n",
    "    \n",
    "    def __init__(self, inputCols, outputCols):\n",
    "        super().__init__(inputCols, outputCols)\n",
    "    \n",
    "    # Transform method to fill missing values\n",
    "    def _transform(self, dataset: DataFrame):\n",
    "        topCategorys = self.getTopCategorys()\n",
    "        outputCols = self.getOutputCols()\n",
    "\n",
    "        updateMissingValue = dict(zip(outputCols, topCategorys))\n",
    "\n",
    "        inputCols = self.getInputCols()\n",
    "        \n",
    "        for outputColumn, inputColumn in zip(outputCols, inputCols):\n",
    "            dataset = dataset.withColumn(outputColumn, col(inputColumn))\n",
    "            # print(dataset.columns)\n",
    "            # print(outputColumn, inputColumn)\n",
    "\n",
    "        dataset = dataset.na.fill(updateMissingValue)\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql.types import TimestampType, StringType, FloatType, StructType, StructField\n",
    "from finance_complaint.exception import FinanceException\n",
    "import os, sys\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "\n",
    "class FinanceDataSchema:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.col_company_response: str = 'company_response'\n",
    "        self.col_consumer_consent_provided: str = 'consumer_consent_provided'\n",
    "        self.col_submitted_via = 'submitted_via'\n",
    "        self.col_timely: str = 'timely'\n",
    "        self.col_diff_in_days: str = 'diff_in_days'\n",
    "        self.col_company: str = 'company'\n",
    "        self.col_issue: str = 'issue'\n",
    "        self.col_product: str = 'product'\n",
    "        self.col_state: str = 'state'\n",
    "        self.col_zip_code: str = 'zip_code'\n",
    "        self.col_consumer_disputed: str = 'consumer_disputed'\n",
    "        self.col_date_sent_to_company: str = \"date_sent_to_company\"\n",
    "        self.col_date_received: str = \"date_received\"\n",
    "        self.col_complaint_id: str = \"complaint_id\"\n",
    "        self.col_sub_product: str = \"sub_product\"\n",
    "        self.col_complaint_what_happened: str = \"complaint_what_happened\"\n",
    "        self.col_company_public_response: str = \"company_public_response\"\n",
    "\n",
    "    @property\n",
    "    def dataframe_schema(self) -> StructType:\n",
    "        try:\n",
    "            schema = StructType([\n",
    "                StructField(self.col_company_response, StringType()),\n",
    "                StructField(self.col_consumer_consent_provided, StringType()),\n",
    "                StructField(self.col_submitted_via, StringType()),\n",
    "                StructField(self.col_timely, StringType()),\n",
    "                StructField(self.col_date_sent_to_company, TimestampType()),\n",
    "                StructField(self.col_date_received, TimestampType()),\n",
    "                StructField(self.col_company, StringType()),\n",
    "                StructField(self.col_issue, StringType()),\n",
    "                StructField(self.col_product, StringType()),\n",
    "                StructField(self.col_state, StringType()),\n",
    "                StructField(self.col_zip_code, StringType()),\n",
    "                StructField(self.col_consumer_disputed, StringType()),\n",
    "\n",
    "            ])\n",
    "            return schema\n",
    "\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys) from e\n",
    "\n",
    "    @property\n",
    "    def target_column(self) -> str:\n",
    "        return self.col_consumer_disputed\n",
    "\n",
    "    @property\n",
    "    def one_hot_encoding_features(self) -> List[str]:\n",
    "        features = [\n",
    "            self.col_company_response,\n",
    "            self.col_consumer_consent_provided,\n",
    "            self.col_submitted_via,\n",
    "        ]\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def im_one_hot_encoding_features(self) -> List[str]:\n",
    "        return [f\"im_{col}\" for col in self.one_hot_encoding_features]\n",
    "\n",
    "    @property\n",
    "    def string_indexer_one_hot_features(self) -> List[str]:\n",
    "        return [f\"si_{col}\" for col in self.one_hot_encoding_features]\n",
    "\n",
    "    @property\n",
    "    def tf_one_hot_encoding_features(self) -> List[str]:\n",
    "        return [f\"tf_{col}\" for col in self.one_hot_encoding_features]\n",
    "\n",
    "    @property\n",
    "    def tfidf_features(self) -> List[str]:\n",
    "        features = [\n",
    "            self.col_issue\n",
    "        ]\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def derived_input_features(self) -> List[str]:\n",
    "        features = [\n",
    "            self.col_date_sent_to_company,\n",
    "            self.col_date_received\n",
    "        ]\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def derived_output_features(self) -> List[str]:\n",
    "        return [self.col_diff_in_days]\n",
    "\n",
    "    @property\n",
    "    def numerical_columns(self) -> List[str]:\n",
    "        return self.derived_output_features\n",
    "\n",
    "    @property\n",
    "    def im_numerical_columns(self) -> List[str]:\n",
    "        return [f\"im_{col}\" for col in self.numerical_columns]\n",
    "\n",
    "    @property\n",
    "    def tfidf_feature(self) -> List[str]:\n",
    "        return [self.col_issue]\n",
    "\n",
    "    @property\n",
    "    def tf_tfidf_features(self) -> List[str]:\n",
    "        return [f\"tf_{col}\" for col in self.tfidf_feature]\n",
    "\n",
    "    @property\n",
    "    def input_features(self) -> List[str]:\n",
    "        in_features = self.tf_one_hot_encoding_features + self.im_numerical_columns + self.tf_tfidf_features\n",
    "        return in_features\n",
    "\n",
    "    @property\n",
    "    def required_columns(self) -> List[str]:\n",
    "        features = [self.target_column] + self.one_hot_encoding_features + self.tfidf_features + \\\n",
    "                    [self.col_date_sent_to_company, self.col_date_received]\n",
    "                    \n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def required_prediction_columns(self) -> List[str]:\n",
    "        features =  self.one_hot_encoding_features + self.tfidf_features + \\\n",
    "                    [self.col_date_sent_to_company, self.col_date_received]\n",
    "                    \n",
    "        return features\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def unwanted_columns(self) -> List[str]:\n",
    "        features = [\n",
    "            self.col_complaint_id,\n",
    "            self.col_sub_product, self.col_complaint_what_happened\n",
    "            ]\n",
    "\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def vector_assembler_output(self) -> str:\n",
    "        return \"va_input_features\"\n",
    "\n",
    "    @property\n",
    "    def scaled_vector_input_features(self) -> str:\n",
    "        return \"scaled_input_features\"\n",
    "\n",
    "    @property\n",
    "    def target_indexed_label(self) -> str:\n",
    "        return f\"indexed_{self.target_column}\"\n",
    "\n",
    "    @property\n",
    "    def prediction_column_name(self) -> str:\n",
    "        return \"prediction\"\n",
    "\n",
    "    @property\n",
    "    def prediction_label_column_name(self) -> str:\n",
    "        return f\"{self.prediction_column_name}_{self.target_column}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('company_response', StringType(), True), StructField('consumer_consent_provided', StringType(), True), StructField('submitted_via', StringType(), True), StructField('timely', StringType(), True), StructField('date_sent_to_company', TimestampType(), True), StructField('date_received', TimestampType(), True), StructField('company', StringType(), True), StructField('issue', StringType(), True), StructField('product', StringType(), True), StructField('state', StringType(), True), StructField('zip_code', StringType(), True), StructField('consumer_disputed', StringType(), True)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = FinanceDataSchema()\n",
    "schema.dataframe_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'consumer_disputed'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['consumer_disputed',\n",
       " 'company_response',\n",
       " 'consumer_consent_provided',\n",
       " 'submitted_via',\n",
       " 'issue',\n",
       " 'date_sent_to_company',\n",
       " 'date_received']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.required_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['company_response',\n",
       " 'consumer_consent_provided',\n",
       " 'submitted_via',\n",
       " 'issue',\n",
       " 'date_sent_to_company',\n",
       " 'date_received']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.required_prediction_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tf_company_response',\n",
       " 'tf_consumer_consent_provided',\n",
       " 'tf_submitted_via',\n",
       " 'im_diff_in_days',\n",
       " 'tf_issue']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_sent_to_company', 'date_received']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.derived_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diff_in_days']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.derived_output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diff_in_days']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im_diff_in_days']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.im_numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['company_response', 'consumer_consent_provided', 'submitted_via']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.one_hot_encoding_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im_company_response', 'im_consumer_consent_provided', 'im_submitted_via']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.im_one_hot_encoding_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['si_company_response', 'si_consumer_consent_provided', 'si_submitted_via']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.string_indexer_one_hot_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['issue']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.tfidf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, OneHotEncoder, StringIndexer, Imputer, \\\n",
    "    IDF, Tokenizer, HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_941a87809752"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_data_transformation_pipeline():\n",
    "    stages = []\n",
    "    \n",
    "    # generating additional column\n",
    "    derived_feature = DerivedFeatureGenerator(inputCols=schema.derived_input_features, \n",
    "                                                outputCols=schema.derived_output_features)\n",
    "    stages.append(derived_feature)\n",
    "    \n",
    "    # creating imputer to fill null values\n",
    "    imputer = Imputer(inputCols=schema.numerical_columns,\n",
    "                        outputCols=schema.im_numerical_columns)\n",
    "    stages.append(imputer)\n",
    "    \n",
    "    # Frequency imputer for categorical features\n",
    "    frequency_imputer = FrequencyImputer(inputCols=schema.one_hot_encoding_features,\n",
    "                                            outputCols=schema.im_one_hot_encoding_features)\n",
    "    stages.append(frequency_imputer)\n",
    "    \n",
    "    # StringIndexer for one-hot encoding features\n",
    "    for im_feature, indexer_col in zip(schema.im_one_hot_encoding_features, schema.string_indexer_one_hot_features):\n",
    "        string_indexer = StringIndexer(inputCol=im_feature, outputCol=indexer_col)\n",
    "        stages.append(string_indexer)\n",
    "    \n",
    "    # OneHotEncoder for indexed categorical features\n",
    "    one_hot_encoder = OneHotEncoder(inputCols=schema.string_indexer_one_hot_features,\n",
    "                                    outputCols=schema.tf_one_hot_encoding_features)\n",
    "    stages.append(one_hot_encoder)\n",
    "    \n",
    "    # Tokenizer for text features\n",
    "    tokenizer = Tokenizer(inputCol=schema.tfidf_features[0], outputCol=\"words\")\n",
    "    stages.append(tokenizer)\n",
    "    \n",
    "    # HashingTF for tokenized words\n",
    "    hashing_tf = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=40)\n",
    "    stages.append(hashing_tf)\n",
    "    \n",
    "    # IDF for term frequency features\n",
    "    idf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=schema.tf_tfidf_features[0])\n",
    "    stages.append(idf)\n",
    "\n",
    "    # VectorAssembler to assemble input features into a single vector\n",
    "    vector_assembler = VectorAssembler(inputCols=schema.input_features,\n",
    "                                        outputCol=schema.vector_assembler_output)\n",
    "    stages.append(vector_assembler)\n",
    "\n",
    "    # StandardScaler to scale the vector of input features\n",
    "    standard_scaler = StandardScaler(inputCol=schema.vector_assembler_output,\n",
    "                                        outputCol=schema.scaled_vector_input_features)\n",
    "    stages.append(standard_scaler)\n",
    "    \n",
    "    # Creating the pipeline with all stages\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Run the function to get the pipeline\n",
    "get_data_transformation_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FinanceData:\n",
    "    \n",
    "    def __init__(self, data_path: str, schema=FinanceDataSchema()):\n",
    "        self.data_path = data_path\n",
    "        self.schema = schema\n",
    "    \n",
    "    def read_data(self):\n",
    "        file_path = self.data_path\n",
    "        dataframe: DataFrame = spark_session.read.parquet(file_path)\n",
    "        \n",
    "        dataframe.printSchema()\n",
    "        \n",
    "        return dataframe\n",
    "    \n",
    "    \n",
    "    def get_balanced_shuffled_dataframe(self, dataframe: DataFrame):\n",
    "        \n",
    "        count_of_each_cat = dataframe.groupby(self.schema.target_column).count().collect()\n",
    "        label = []\n",
    "        n_record = []\n",
    "        \n",
    "        for info in count_of_each_cat:\n",
    "            n_record.append(info['count'])\n",
    "            label.append(info[self.schema.target_column])\n",
    "\n",
    "        minority_row = min(n_record)\n",
    "        n_per = [minority_row / record for record in n_record]\n",
    "\n",
    "        selected_row = []\n",
    "        for label, per in zip(label, n_per):\n",
    "            print(label, per)\n",
    "            temp_df, _ = dataframe.filter(col(self.schema.target_column) == label).randomSplit([per, 1 - per])\n",
    "            selected_row.append(temp_df)\n",
    "\n",
    "        selected_df: DataFrame = None\n",
    "        for df in selected_row:\n",
    "            df.groupby(self.schema.target_column).count().show()\n",
    "            \n",
    "            if selected_df is None:\n",
    "                selected_df = df\n",
    "            else:\n",
    "                selected_df = selected_df.union(df)\n",
    "\n",
    "        selected_df = selected_df.orderBy(rand())\n",
    "\n",
    "        selected_df.groupby(self.schema.target_column).count().show()\n",
    "        \n",
    "        return selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(dataframe: DataFrame, metric_name, label_col, prediction_col) -> float:\n",
    "    try:\n",
    "        evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol=label_col, predictionCol=prediction_col,\n",
    "            metricName=metric_name)\n",
    "        score = evaluator.evaluate(dataframe)\n",
    "        print(f\"{metric_name} score: {score}\")\n",
    "        \n",
    "        #logger.info(f\"{metric_name} score: {score}\")\n",
    "        return score\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise FinanceException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelTrainerConfig = namedtuple(\"ModelTrainerConfig\", [\"base_accuracy\", \"trained_model_file_path\", \"metric_list\",\n",
    "                                                        'label_indexer_model_dir', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_scores(dataframe: DataFrame, metric_names: List[str]) -> List[tuple]:\n",
    "    if metric_names is None:\n",
    "        metric_names = ModelTrainerConfig.metric_list\n",
    "\n",
    "    scores: List[tuple] = []\n",
    "    for metric_name in metric_names:\n",
    "        score = get_score(metric_name=metric_name,\n",
    "                        # A keyword argument.\n",
    "                        dataframe=dataframe,\n",
    "                        label_col=schema.target_indexed_label,\n",
    "                        prediction_col=schema.prediction_column_name, \n",
    "                        )\n",
    "        \n",
    "        scores.append((metric_name, score))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, StringIndexerModel\n",
    "from pyspark.ml.feature import IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(self, label_indexer_model: StringIndexerModel) -> Pipeline:\n",
    "    stages = []\n",
    "    #logger.info(\"Creating Random Forest Classifier class.\")\n",
    "    \n",
    "    random_forest_clf = RandomForestClassifier(labelCol=self.schema.target_indexed_label,\n",
    "                                                featuresCol=self.schema.scaled_vector_input_features)\n",
    "\n",
    "    #logger.info(\"Creating Label generator\")\n",
    "    \n",
    "    label_generator = IndexToString(inputCol=self.schema.prediction_column_name,\n",
    "                                    outputCol=f\"{self.schema.prediction_column_name}_{self.schema.target_column}\",\n",
    "                                    labels=label_indexer_model.labels)\n",
    "    stages.append(random_forest_clf)\n",
    "    stages.append(label_generator)\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
